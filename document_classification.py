# -*- coding: utf-8 -*-
"""Document Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tLJEEs0XrIOIpZsGj32Tv3W4piGeQX9G
"""

from sklearn.datasets import fetch_20newsgroups

newsgroups = fetch_20newsgroups(subset='all')# import all subsets from fetch_20newsgroups including training testing and validation sets
newsgroups.keys()
# newsgroups is a dictionary with keys = ['data', 'filenames', 'target_names', 'target', 'DESCR']

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter

categories = newsgroups.target_names
labels = newsgroups.target
# Using np.bincount to count the number of occurrences for each label
sample_counts = np.bincount(labels)
# np.bincount counts the occurrences of non-negative integers in an array.
# In this case, labels is an array of integers (0, 1, 2, ..., representing categories), and np.bincount will return an array where each index represents a category, and the value at each index is the count of occurrences for that category.

# Convert the np.bincount result into a DataFrame
data = {'Category': categories, 'Sample Count': sample_counts}
df = pd.DataFrame(data)

# plot a barplot
sns.barplot(data = df,x='Sample Count',y='Category')
plt.title('Sample Count in Each Category')
plt.xlabel('Category')
plt.ylabel('Sample Count')
plt.show()
# Balanced Dataset:

# Most of the categories have relatively similar sample counts, ranging from around 500 to nearly 1000. This indicates that the dataset is fairly balanced across categories.
# Categories with More Samples:

# The categories "talk.religion.misc", "sci.space", and "talk.politics.mideast" have slightly more samples than others, approaching or surpassing 900 samples.
# Categories with Fewer Samples:

# "soc.religion.christian" and "talk.politics.guns" have visibly fewer samples, falling below 600.
# These could potentially represent underrepresented topics in this dataset, meaning there might be fewer training examples for classifiers to learn from in these categories.
# Possible Class Imbalance:

# Even though the differences aren't drastic, certain categories like "soc.religion.christian" might introduce a slight imbalance. This is something to consider if you're training machine learning models because imbalances can affect model performance.
# Consistency Across Most Categories:

# Other than the few exceptions mentioned, the majority of categories have sample counts in a narrow range, which is helpful for training models that expect roughly equal representation of classes.

import re
import string
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import Pipeline

from sklearn.model_selection import train_test_split
from sklearn.base import TransformerMixin, BaseEstimator
from sklearn.preprocessing import FunctionTransformer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
import nltk

# Download necessary tokenizer
nltk.download('punkt')


X = newsgroups.data
y = newsgroups.target

# Custom transformer for text preprocessing
class TextPreprocessor(BaseEstimator, TransformerMixin):
    def __init__(self):
        self.stemmer = PorterStemmer()

    def preprocess_text(self, text):
        # Lowercase
        text = text.lower()
        # Remove punctuation
        text = text.translate(str.maketrans('', '', string.punctuation))
        # Remove numbers
        text = re.sub(r'\d+', '', text)
        # Tokenize and stem
        tokens = word_tokenize(text)
        stemmed_tokens = [self.stemmer.stem(token) for token in tokens]
        # Join tokens back into a single string
        return ' '.join(stemmed_tokens)

    def fit(self, X, y=None):
        return self

    def transform(self, X, y=None):
        return [self.preprocess_text(doc) for doc in X]

from sklearn.naive_bayes import MultinomialNB

from sklearn.metrics import accuracy_score, classification_report
# Define the pipeline with Naive Bayes
pipeline = Pipeline([
    # Step 1: Text Preprocessing
    ('preprocess', TextPreprocessor()),

    # Step 2: Convert text to TF-IDF vectors
    ('vectorize', TfidfVectorizer(stop_words='english')),

    # Step 3: Naive Bayes Classifier
    ('classifier', MultinomialNB())
])

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)

# Fit the pipeline to the training data
pipeline.fit(X_train, y_train)

# Predict on the test data using the fitted model
y_pred = pipeline.predict(X_test)

# Evaluate the model performance
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred, target_names=newsgroups.target_names))

# Accuracy: 0.8761273209549072
#                             precision    recall  f1-score   support

#              alt.atheism       0.86      0.79      0.83       160
#            comp.graphics       0.83      0.87      0.85       195
#  comp.os.ms-windows.misc       0.86      0.86      0.86       197
# comp.sys.ibm.pc.hardware       0.74      0.85      0.79       196
#    comp.sys.mac.hardware       0.87      0.85      0.86       193
#           comp.windows.x       0.92      0.87      0.90       198
#             misc.forsale       0.92      0.73      0.81       195
#                rec.autos       0.92      0.91      0.92       198
#          rec.motorcycles       0.96      0.97      0.97       199
#       rec.sport.baseball       0.97      0.97      0.97       199
#         rec.sport.hockey       0.96      0.97      0.97       200
#                sci.crypt       0.80      0.98      0.88       198
#          sci.electronics       0.90      0.83      0.86       197
#                  sci.med       0.97      0.94      0.96       198
#                sci.space       0.95      0.96      0.96       197
#   soc.religion.christian       0.66      0.96      0.78       199
#       talk.politics.guns       0.81      0.96      0.87       182
#    talk.politics.mideast       0.93      0.98      0.95       188
#       talk.politics.misc       0.97      0.74      0.84       155
#       talk.religion.misc       1.00      0.23      0.37       126

#                 accuracy                           0.88      3770
#                macro avg       0.89      0.86      0.86      3770
#             weighted avg       0.89      0.88      0.87      3770